@article{Alibali2005,
abstract = {Do hand gestures play a role in spatial cognition? This paper reviews literature addressing the roles of gestures in (1) expressing spatial information, (2) communicating about spatial information, and (3) thinking about spatial information. Speakers tend to produce gestures when they produce linguistic units that contain spatial information, and they gesture more when talking about spatial topics than when talking about abstract or verbal ones. Thus, gestures are commonly used to express spatial information. Speakers use gestures more in situations when those gestures could contribute to communication, suggesting that they intend those gestures to communicate. Further, gestures influence addressees' comprehension of the speech they accompany, and addressees also detect information that is conveyed uniquely in gestures. Thus, gestures contribute to effective communication of spatial information. Gestures also play multiple roles in thinking about spatial information. There is evidence that gestures activate lexical and spatial representations, promote a focus on spatial information, and facilitate the packaging of spatial information in speech. Finally, some of the observed variation across tasks in gesture production is associated with task differences in demands on spatial cognitive processes, and individual differences in gesture production are associated with individual differences in spatial and verbal abilities. In sum, gestures appear to play multiple roles in spatial cognition. Central challenges for future research include: (1) better specification of the mental representations that give rise to gestures, (2) deeper understanding of the mechanisms by which gestures play a role in spatial thinking, and (3) greater knowledge of the sources of task and individual differences in gesture production.},
author = {Alibali, Mw},
doi = {10.1207/s15427633scc0504},
file = {:home/tilman/Programming/MMK/MultimodalPaper/Spatial Cognition and Computation Volume 5 issue 4 2005 [doi 10.1207{\%}2Fs15427633scc0504{\_}2] Alibali, Martha W. -- Gesture in Spatial Cognition- Expressing, Communicating, and Thinking About Spatial Inf.pdf:pdf},
isbn = {Spatial Cognition and Computation, Vol. 5, No. 4, December 2005, pp. 307–331},
issn = {13875868},
journal = {Spatial Cognition and Computation},
keywords = {communication,gestures,spatial representations},
number = {4},
pages = {307--331},
pmid = {21635310},
title = {{Gesture in spatial cognition: Expressing, communicating, and thinking about spatial information}},
url = {http://www.tandfonline.com/doi/abs/10.1207/s15427633scc0504{\_}2},
volume = {5},
year = {2005}
}
@incollection{Allwood2008,
author = {Allwood, Jens},
booktitle = {Corpus Linguistics. An International Handbook},
editor = {L{\"{u}}deling, A and Kyt{\"{o}}, M},
file = {:home/tilman/Programming/MMK/MultimodalPaper/mulitimodal{\_}corpora.pdf:pdf},
keywords = {Computational linguistics,Corpus linguistics},
pages = {207--225},
publisher = {Mouton de Gruyter},
title = {{Multimodal Corpora}},
year = {2008}
}
@misc{BAS2014,
author = {{Bayerisches Archiv f{\"{u}}r Sprachsignale (BAS)}},
title = {{Bielefeld Speech and Gesture Alignment Corpus - SaGA}},
url = {https://www.phonetik.uni-muenchen.de/Bas/BasSaGAdeu.html},
urldate = {2016-12-25},
year = {2014}
}
@misc{Bergmann2014,
author = {Bergmann, Kirsten and Damm, Oliver and Freigang, Farina and Hahn, Florian and Kopp, Stefan and Letetzki, Julia and Rieser, Hannes and Thomas, Nick and Wittwer, Nicole},
file = {:home/tilman/Programming/MMK/MultimodalPaper/BasSaGADoku.pdf:pdf},
title = {{Documentation - Sagaland}},
year = {2014}
}
@article{Caldognetto2004,
author = {Caldognetto, Emanuela Magno and Poggi, Isabella and Cosi, Piero and Cavicchio, Federica and Merola, G.},
file = {:home/tilman/Programming/MMK/MultimodalPaper/Swedish{\_}PF{\_}Star.pdf:pdf},
journal = {International Conference on Language Resources and Evaluation Workshop on Multimodal Corpora},
pages = {29--33},
title = {{Multimodal Score: an ANVIL™ Based Annotation Scheme for Multimodal Audio-Video Analysis}},
year = {2004}
}
@article{Hahn2010,
author = {Hahn, Florian and Rieser, Hannes},
file = {:home/tilman/Programming/MMK/MultimodalPaper/13.pdf:pdf},
journal = {Dialogue},
keywords = {a,a one-dimensional,completion,for example production of,gesture typology,gesture-,partial ontology,saga,speech interface,the types of gestures,used and their function},
pages = {99--109},
title = {{Explaining Speech Gesture Alignment in MM Dialogue Using Gesture Typology}},
volume = {1},
year = {2010}
}
@book{Kipp2009,
author = {Kipp, M and Martin, JC and Paggio, P and Heylen, D},
editor = {Goebel, R and Siekmann, J and Wahlster, W},
file = {:home/tilman/Programming/MMK/MultimodalPaper/Multimodal Corpora Book:},
isbn = {9783642047923},
publisher = {Springer-Verlag Berlin Heidelberg},
title = {{Multimodal Corpora}},
year = {2009}
}
@article{Bielefeld2013,
abstract = {People communicate multimodally. Most prominently, they co-produce speech and gesture. How do they do that? Studying the interplay of both modalities has to be informed by empirically observed communication behavior. We present a corpus built of speech and gesture data gained in a controlled study. We describe 1) the setting underlying the data; 2) annotation of the data; 3) reliability evalution methods and results; and 4) applications of the corpus in the research domain of speech and gesture alignment.},
author = {L{\"{u}}cking, Andy and Bergman, Kirsten and Hahn, Florian and Kopp, Stefan and Rieser, Hannes},
doi = {10.1007/s12193-012-0106-8},
file = {:home/tilman/Programming/MMK/MultimodalPaper/Journal on Multimodal User Interfaces Volume 7 issue 1-2 2013 [doi 10.1007{\%}2Fs12193-012-0106-8] Andy L{\"{u}}cking, Kirsten Bergman, Florian Hahn{\ldots} -- Data-based analysis of speech and gesture- the Bielefeld.pdf:pdf},
issn = {17837677},
journal = {Journal on Multimodal User Interfaces},
keywords = {Iconic gesture,Multimodal data,Multimodal dialogue,Multimodal simulation,Speech-and-gesture alignment},
number = {1-2},
pages = {5--18},
title = {{Data-based analysis of speech and gesture: The Bielefeld Speech and Gesture Alignment corpus (SaGA) and its applications}},
volume = {7},
year = {2013}
}
@inproceedings{Bielefeld2010,
abstract = {People communicate multimodally. Most prominently, they co-produce speech and gesture. How do they do that? Studying the interplay of both modalities has to be informed by empirically observed communication behavior. We present a corpus built of speech and gesture data gained in a controlled study. We describe 1) the setting underlying the data; 2) annotation of the data; 3) reliability evalution methods and results; and 4) applications of the corpus in the research domain of speech and gesture alignment.},
author = {L{\"{u}}cking, Andy and Bergmann, Kirsten and Hahn, Florian and Kopp, Stefan and Rieser, Hannes},
booktitle = {LREC 2010 Workshop: Multimodal Corpora–Advances in Capturing, Coding and Analyzing Multimodality},
file = {:home/tilman/Programming/MMK/MultimodalPaper/Bielefeld.pdf:pdf},
pages = {92----98},
title = {{The Bielefeld Speech and Gesture Alignment Corpus ( SaGA )}},
year = {2010}
}
@article{wagner:2011:1,
 author = {Johannes Wagner and Florian Lingenfelser and Elisabeth Andr{\'e} and Jonghwa Kim},
 title = {Exploring Fusion Methods for Multimodal Emotion Recognition with Missing Data},
 journal ={IEEE Transactions on Affective Computing},
 volume = {99},
 number = {PrePrints},
 issn = {1949-3045},
 year = {2011},
 publisher = {IEEE Computer Society},
 address = {Los Alamitos, CA, USA}
}
@inproceedings{ 2010MultimodalCorpusForGestureExpressivity,
author = "G. Caridakis, Johannes Wagner, A. Raouzaiou, Z. Curto, Elisabeth Andre and K. Karpouzis",
title = "A multimodal corpus for gesture expressivity analysis",
booktitle = "Multimodal Corpora: Advances in Capturing, Coding and Analyzing Multimodality, LREC",
year = 2010,
editor = "G. Caridakis, J. Wagner, A. Raouzaiou, Z. Curto, E. Andre, K. Karpouzis",
address = "Malta",
month = 5,
}
@Article{Caridakis2013,
author="Caridakis, G.
and Wagner, J.
and Raouzaiou, A.
and Lingenfelser, F.
and Karpouzis, K.
and Andre, E.",
title="A cross-cultural, multimodal, affective corpus for gesture expressivity analysis",
journal="Journal on Multimodal User Interfaces",
year="2013",
volume="7",
number="1",
pages="121--134",
abstract="A multimodal, cross-cultural corpus of affective behavior is presented in this research work. The corpus construction process, including issues related to the design and implementation of an experiment, is discussed along with resulting acoustic prosody, facial expressions and gesture expressivity features. However, research work presented here focuses more on the cross-cultural aspect of gestural behavior defining a common corpus construction protocol aiming to identify cultural patterns within non-verbal behavior across cultures i.e. German, Greek and Italian. Culture specific findings regarding gesture expressivity are derived from the affective analysis performed. Additionally, the multimodal aspect, including prosody and facial expressions, is researched in terms of fusion techniques. Finally, a release plan of the corpus to the public domain is discussed aiming to establish the current corpus as a benchmark multimodal, cross-cultural standard and reference point.",
issn="1783-8738",
doi="10.1007/s12193-012-0112-x",
url="http://dx.doi.org/10.1007/s12193-012-0112-x"
}
@article{VeltenJr1968473,
title = "A laboratory task for induction of mood states ",
journal = "Behaviour Research and Therapy ",
volume = "6",
number = "4",
pages = "473 - 482",
year = "1968",
note = "",
issn = "0005-7967",
doi = "http://dx.doi.org/10.1016/0005-7967(68)90028-4",
url = "http://www.sciencedirect.com/science/article/pii/0005796768900284",
author = "Emmett Velten Jr.",
abstract = "One hundred female college students were administered the Harvard Group Scale of Hypnotic Susceptibility, Form A, to provide a measure of primary suggestibility. In a 2nd hr, each S was randomly assigned to one of five individual treatments of 20 Ss each. One group read and concentrated upon 60 self-referent statements intended to be elating: a second group read 60 statements intended to be depressing. A third group read 60 statements which were neither self-referent nor pertaining to mood. This group controlled for the effects of reading and experimental participation per se. Fourth and fifth groups received demand characteristics control treatments designed to produce simulated elation and simulated depression, respectively. Two measures of pre-treatment mood level were obtained from each S at the beginning other individual treatment. Following treatment, as criteria for elation and depression, seven behavioral task measures were obtained. Four of these distinguished significantly among the treatment groups. The comparative performance of Ss in the three control groups indicated that the obtained mood changes could not be attributed to artifactual effects. Moreover, post-experimental questionnaire data strongly supported the conclusion that Elation and Depression treatments had indeed respectively induced elation and depression. "
}
@article{SmartSensorIntegration2010,
author = {},
title = {Smart sensor integration: A framework for multimodal emotion recognition in real-time},
journal = {2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops (ACII 2009)},
volume = {00},
number = {undefined},
issn = {2156-8103},
year = {2010},
pages = {1-8},
doi = {doi.ieeecomputersociety.org/10.1109/ACII.2009.5349571},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
}

@inproceedings{schiel2002smartkom,
  title={The SmartKom Multimodal Corpus at BAS.},
  author={Schiel, Florian and Steininger, Silke and T{\"u}rk, Ulrich},
  booktitle={LREC},
  year={2002}
}
@online{basSKP,
ALTauthor = {author},
ALTeditor = {editor},
title = {BAS Bavarian Archive for Speech Signals SmartKom - SKP},
date = {16.01.17},
url = {http://www.bas.uni-muenchen.de/forschung/Bas/BasSmartKomPubliceng.html},
OPTsubtitle = {subtitle},
OPTtitleaddon = {titleaddon},
OPTlanguage = {language},
OPTversion = {version},
OPTnote = {note},
OPTorganization = {organization},
OPTdate = {date},
OPTmonth = {month},
OPTyear = {year},
OPTaddendum = {addendum},
OPTpubstate = {pubstate},
OPTurldate = {urldate},
}

@online{basAUDIO,
ALTauthor = {author},
ALTeditor = {editor},
title = {BAS Bavarian Archive for Speech Signals SmartKom - SKAUDIO},
date = {16.01.17},
url = {http://www.bas.uni-muenchen.de/forschung/Bas/BasSmartKomAudioeng.html},
OPTsubtitle = {subtitle},
OPTtitleaddon = {titleaddon},
OPTlanguage = {language},
OPTversion = {version},
OPTnote = {note},
OPTorganization = {organization},
OPTdate = {date},
OPTmonth = {month},
OPTyear = {year},
OPTaddendum = {addendum},
OPTpubstate = {pubstate},
OPTurldate = {urldate},
}


@inproceedings{schiel2002integration,
  title={Integration of multi-modal data and annotations into a simple extendable form: the extension of the BAS Partitur Format},
  author={Schiel, Florian and Steininger, Silke and Beringer, Nicole and T{\"u}rk, Ulrich and Rabold, Susen},
  booktitle={Proc. of the 3rd Int. conf. on Language Resources and Evaluation, Workshop On Multimodal Resources And Multomodal Systems Evaluation, Las Palmas, Spain},
  year={2002},
  organization={Citeseer}
}

@inproceedings{wahlster2003towards,
  title={Towards symmetric multimodality: Fusion and fission of speech, gesture, and facial expression},
  author={Wahlster, Wolfgang},
  booktitle={Annual Conference on Artificial Intelligence},
  pages={1--18},
  year={2003},
  organization={Springer}
}