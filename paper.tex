\documentclass[a4paper]{article}

\usepackage{INTERSPEECH2016}

\usepackage{graphicx}
\usepackage{amssymb,amsmath,bm}
\usepackage{textcomp}
\usepackage{todo}
\usepackage{glossaries}
\usepackage[colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue]{hyperref}
\usepackage{varioref}

\labelformat{section}{Section~#1}
\labelformat{subsection}{Section~#1}
\labelformat{subsubsection}{Section~#1}
\labelformat{equation}{Eq.~#1}
\labelformat{figure}{Figure~#1}
\labelformat{table}{Table~#1}

\def\vec#1{\ensuremath{\bm{{#1}}}}
\def\mat#1{\vec{#1}}


\sloppy % better line breaks
\ninept

\title{Multimodal Corpora}

% Authors
\makeatletter
\def\name#1{\gdef\@name{#1\\}}
\makeatother
\name{{\em Tuan Pham Minh, Nils Harder, Tilman Krokotsch}}

% Affilations and Contact information
\address{Otto-von-Guericke-University Magdeburg \\
  Faculty of Computer Science \\
  Faculty of Electrical Engineering and Information Technology \\
  {\small \tt \{tuan.pham, nils.harder, tilman.krokotsch\}@st.ovgu.de}
}

%Acronyms
\newacronym{saga}{SaGA}{Bielefeld Speech and Gesture Alignment}
\newacronym{dfg}{DFG}{German Research Foundation}
\newacronym{lrec}{LREC}{Language Resources and Evaluation Conference}
\newacronym{vr}{VR}{Virtual Reality}
\newacronym{asl}{ASL}{American Sign Language}

\newacronym{callas}{Callas}{Conveying Affectiveness in Leading edge Living Adaptive Systems}
%
\begin{document}

	\maketitle
  	%
  	\begin{abstract}
    	This paper is a short summary of three multimodal corpora for the course MMK in the winter term 16/17 at the Otto-von-Guericke-University Magdeburg. The concept and aim of multimodal corpora is explained. Afterwards three exemplary corpora are described. This includes the recording process and the proposed application context, as well as the quality and quantity of the data.
  	\end{abstract}
  	\noindent{\bf Index Terms}: human machine interaction, multimodal corpora, computional linguistics
  	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
  	\section{Introduction}
		\Todo{Add definitions and intro to multimodal corpora}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	\section{First Corpus}
		\Todo{Add first corpus}
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	
	
	\section{The CALLAS Expressivity Corpus}
		The yet to be released CALLAS Expressivity Corpus is part of the \gls{callas} project \cite{2010MultimodalCorpusForGestureExpressivity}. It was introduced in 2010 \cite{2010MultimodalCorpusForGestureExpressivity} and further described in \cite{Caridakis2013} three years later. All the information in this section is taken from those two papers if not stated otherwise, as this corpus was not released to the public yet, so there is no other information source besides a paper from one of the collaborators\cite{wagner:2011:1}. This corpus was a collaboratiove work between the National Technical University of Athens (G. Caridakis, A. Raouzaiou, K. Karpouzis), the Augsburg University (J. Wagner, F. Lingenfelser, E. Andre) and Humanware S.r.l. (Z. Curto). The available modalities are audio, video and hand gestures recorded separately in three different ways. 
		\subsection{Motivation}
			The main focus for this corpus was to capture cross-cultural differences in expressing emotoions with hand gestures. Obtaining the data which is needed to investigate the differences in cultures was done by collecting data in Greece, Germany and Italy as well. Another motivation was the recording of material which is as realistic as possible. This led the team to use persons without any background as actor, with the result that the data's variance is high, which is better to model the real world as the emotion expression of humans may vary greatly.
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	
	
	\section{The Bielefeld Speech and Gesture Alignment Corpus ( SaGA )}
		The \gls{saga} corpus was produced by a research group at the University of Bielefeld and funded by the \gls{dfg}. It was carried out in the CRC 673 "Alignment in Communication" and published in the proceedings of the workshop "Multimodal Corpora-Advances in Capturing, Coding and Analyzing Multimodality" at the \gls{lrec} in 2010.
		
		The aim of the corpus was to portray the interplay of speech and gesture in face to face conversations. This task was taken on by a interdisicplinary approach. On the one hand the research team took a linguistic perspective to enable Theoretical Linguistic Reconstructions based on the corpus data, and on the other hand took the perspective of computer science to implement the lingusistic findings for multimodal communication in virtual agents and robots. \cite[ch. 1]{Bielefeld2010}
	
		\subsection{Recording Scenario}
			As the goal was to record speech in combination with gestures, the recording scenario had to chosen such that a sufficient amount of gestures was produced by the speaker. Furthermore the gestures should be easily interpretable to aid the annotation process and not externally forced to prevent unnatural ones. Therefore a scenario with the following properties was chosen \cite[ch. 2]{Bielefeld2010}:	
			\begin{itemize}
				\item The topic of the recorded converations was a spatial communication task. The participant had to give directions and describe landmarks. The descision for this topic was based on the evidence for a higher use of gestures, when speaking about spatial topics, provided by \cite[p. 313]{Alibali2005}. This ensured the appropriate amount of easily interpretable gestures.
				\item The stimulus for the participants was a "bus tour" through a virtual city. This was presented in \gls{vr} to make sure ensure the exact same stimulus for each participant. This improved the comparibility of the individual participant data.
				\item After receiving the stimulus, the participant had to describe the "bus tour" and the included landmarks to another person, who has not seen the stimulus. This provided a natural yet controlled conversation scenario for the participant.
			\end{itemize}	
						
		\subsection{Recorded Data}
			\subsubsection{Primary Data}
				The primary data comprises tree synchronized camera perspectives, the audio feed of the conversation, as well as body movement and eye tracking data of the direction giving participant. The camera perspectives include a frontal shot of the direction giver, one of the follower, and one of the whole dialog scenario. The specifications of the primary data given by \cite[ch. 2.1]{Bielefeld2010} and \cite{BAS2014} are summarized in \ref{table:sagaPrimary}. Further technical specifications of the primary data was not available.
				
				\begin{table}
					\center
					\begin{tabular}{|l|c|}
						\hline 
						\textbf{Specification} & \textbf{Value} \\ 
						\hline 
						Number of dialogs & 25 \\ 
						\hline 
						Video time & 280 minutes \\ 
						\hline 
						Video specification & 720x576, 25fps  \\ 
						\hline 
						Video codec & H.264  \\ 
						\hline 
						Video feeds & 3 \\ 
						\hline 
						Audio feeds & 1 \\ 
						\hline
					\end{tabular} 
					\caption{Primary data of the \gls{saga} corpus \cite{Bielefeld2010, BAS2014}}
					\label{table:sagaPrimary}
				\end{table}
				
			\subsubsection{Secondary Data}
				The secondary data of the corpus (\ref{table:sagaSecondary}) contains on the one hand the annotation of the speech and on the other hand the annotation of the gestures. The speech annotation transcribes the utterance on the level of words. Furthermore the utterance is grouped in clauses, each containing a communicative goal \cite[ch. 2]{Bielefeld2010}:
				\begin{itemize}
					\item Naming a landmark
					\item Landmark property description
					\item Landmark construction description
					\item Landmark position description
				\end{itemize}
				
				The gesture annotation has to be explained more in detail. The gestures of both the direction giver and the follower were annotated from two different perspectives. First the gestures were segmented and classified. Afterwards the morphology of the gestures was described. The classifying gesture annotation consists of annotation elements containing \cite[ch. 1.3]{Bergmann2014}:
				
				\begin{itemize}
					\item Annotation tier
					\item start point in time
					\item end point in time
					\item annotation value
				\end{itemize}								
				
				The tiers form a hierachy in the annotation and can contain different annotaion values:
				
				\begin{enumerate}
					\item The top tier is the \textbf{sequence}. A sequence contains several gestures directly following each other. There is no so called resting pose between them. No annotation value is assigned on this tier.
					\item The \textbf{phrase} is the tier of a single gesture. This gesture can have one of the following types:
					\begin{description}
						\item[deictic:] A static pointing gesture often involving the index finger. This gesture does not represent something by itself.
						\item[iconic:] A often dynamic gesture describing the object it represents.
						\item[beat:] A rhythmic gesture constisting of two phases. They are used to emphasize or structure the speech.
						\item[discourse:] A gesture used to structure the dialog between the partners.
						\item[move:] This are mostly unconscious movements depicting the emotional state of the subject.
						\item[mixed forms:] It is possible to assign mixed types consisting of the first three types.
					\end{description}
					\item The phrase can be divided in different \textbf{phases}. Following phases can be distinguished:
					\begin{itemize}
						\item Preparation
						\item Stroke
						\item retraction
						\item pre-hold
						\item post-hold
					\end{itemize}
					\item The \textbf{practice} tier describes the meaning of a gesture by the way it is performed. Move, beat and discourse gestures are not annotated this way. Following practices can be assigned:
					\begin{description}
						\item[shaping] an object in the three-dimensional space.
						\item[sizing] an beforehands described object or depicting a distance.
						\item[modelling] an object. The hand itself is the representation of the object.
						\item[drawing] an object in the two-dimensional space.
						\item[pantomiming] an action the pantomime does himself.
						\item[indexing] an direction or to a position.
						\item[grasping] an object.
						\item[counting] with fingers or showing a number of fingers
						\item[hedging] is a methode displaying uncertainty.
					\end{description}					 
					\item The \textbf{perspective} tier describes the viewpoint from which a path is described by a gesture. If items are arranged relative to the speaker the 'speaker' value is assigned, the 'survey' value otherwise.
					\item The \textbf{referent type} tier contains the type of place, object, action, etc. the gesture referes to. Additionally it contains the \textbf{referent name} tier with the name of the reference.
				\end{enumerate}	
				
				The morphology annotation consists of the same annotation elements as the classifying one. The stroke of a gesture is described by the following properties \cite[ch. 2.2]{Bielefeld2010}
				\begin{itemize}
					\item The \textbf{hand shape} is classified by using a modified lexicon of the \gls{asl}.
					\item The \textbf{hand orientation} is modelled by the orientation of the \textbf{palm} and the \textbf{back of the hand}, the position of the \textbf{wrist} and its distance from the speaker.
					\item For dynamic movements the \textbf{movement path}, the \textbf{direction} and the \textbf{repitition} of the movement for each property are annotated. Additionally curved and linear movements are distinguished.
				\end{itemize}								
			
				\begin{table}
					\center
					\begin{tabular}{|l|c|}
						\hline
						\textbf{Specification} & \textbf{Value} \\ 
						\hline 
						Words & 39,435 \\ 
						\hline 
						Iconic/Deictic gestures & 4961 \\ 
						\hline 
						Discourse gestures & approx. 1000 \\ 
						\hline 
					\end{tabular} 
					\caption{Secondary data of the \gls{saga} corpus \cite{Bielefeld2010}}
				\label{table:sagaSecondary}
				\end{table}
	
		\subsection{Application}
			The authors of the \gls{saga} corpus present two different applications in \cite[ch. 4]{Bielefeld2010}.
			
			The first application is to create a \textbf{typological grid} based on the morphological gesture annotation. The aim of this application was to determine whether the shapes arising from the gestures are systematically used or if they are arbitrary tokens. Furthermore it has to be investigated if this systematic usage is only observed in one dialog, by only one participant or even in the whole corpus. With this additional information about the gestures, forming the connection between the meaning of the spoken language and the gestures can be simplified. The typology provides a more high level way of classifying gestures and gesture sequences. The findings of this research was published in \cite{Hahn2010}.
			
			The second usage of the corpus involves the automatic generation of iconic gestures for virtual Avatars. To do this the \gls{saga} corpus was used to learn \textbf{bayesian networks}. This networks can also be used to compare the gesture production of two individuals. The different propability distributions of the networks enable novel insights in the gesture generation process \cite[ch. 4.2]{Bielefeld2010}.
		
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  	\newpage
  	\eightpt
  	\bibliographystyle{IEEEtran}

  	\bibliography{thebib}

\end{document}
