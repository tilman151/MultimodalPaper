\documentclass[a4paper]{article}

\usepackage{INTERSPEECH2016}

\usepackage{graphicx}
\usepackage{amssymb,amsmath,bm}
\usepackage{textcomp}
\usepackage{todo}
\usepackage{glossaries}
\usepackage[colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue]{hyperref}
\usepackage{varioref}

\labelformat{section}{Section~#1}
\labelformat{subsection}{Section~#1}
\labelformat{subsubsection}{Section~#1}
\labelformat{equation}{Eq.~#1}
\labelformat{figure}{Figure~#1}
\labelformat{table}{Table~#1}

\def\vec#1{\ensuremath{\bm{{#1}}}}
\def\mat#1{\vec{#1}}


\sloppy % better line breaks
\ninept

\title{Multimodal Corpora}

% Authors
\makeatletter
\def\name#1{\gdef\@name{#1\\}}
\makeatother
\name{{\em Tuan Pham Minh, Nils Harder, Tilman Krokotsch}}

% Affilations and Contact information
\address{Otto-von-Guericke-University Magdeburg \\
  Faculty of Computer Science \\
  Faculty of Electrical Engineering and Information Technology \\
  {\small \tt \{tuan.pham, nils.harder, tilman.krokotsch\}@st.ovgu.de}
}

%Acronyms

\newacronym{bas}{BAS}{Bavarian Archive for Speech Signals}
\newacronym{sm}{SM}{SmartKom}

\newacronym{saga}{SaGA}{Bielefeld Speech and Gesture Alignment}
\newacronym{dfg}{DFG}{German Research Foundation}
\newacronym{lrec}{LREC}{Language Resources and Evaluation Conference}
\newacronym{vr}{VR}{Virtual Reality}
\newacronym{asl}{ASL}{American Sign Language}
\newacronym{ssi}{SSI}{Smart Sensor Integration}


\newacronym{callas}{Callas}{Conveying Affectiveness in Leading edge Living Adaptive Systems}
%
\begin{document}

	\maketitle
  	%
  	\begin{abstract}
    	This paper is a short summary of three multimodal corpora for the course MMK in the winter term 16/17 at the Otto-von-Guericke-University Magdeburg. The concept and aim of multimodal corpora is explained. Afterwards three exemplary corpora are described. This includes the recording process and the proposed application context, as well as the quality and quantity of the data.
  	\end{abstract}
  	\noindent{\bf Index Terms}: human machine interaction, multimodal corpora, computional linguistics
  	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
  	\section{Introduction}
		\Todo{Add definitions and intro to multimodal corpora}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	\section{BAS SmartKom Corpus} 
		The \gls{bas} SmartKom Corpus is a multimodal corpus of audio and video data created during the SmartKom-Project at the University of Munich as a Cooperation between Daimler/Chrysler, DFKI, European Media Lab (EML), Philips, Media Interface, Siemens,	SONY, and the Universities of Erlangen, Munich, Stuttgart. \cite{schiel2002smartkom}. The project finished in 2001 and the resulting corpus was released to the public in July 2002. It consists of annotated and synchronized audio and video files of subjects interacting with the SmartKom User-Interface, with 45 subjects and two 4,5 minute sessions per subject.
		\subsection{Motivation}
			 The goal of the SmartKom-Project  was the development of Computer-User-Interfaces enabling natural interaction.The focus was the investigation of possibilities of multi-modal interaction using natural speech input, two-dimensional gestures and facial expressions.
			 A secondary inquiry was into the possibility of user authentication and verification using the available biometric measures(voice, hand contour and signature).
				 
		\subsection{Recording Scenario}
			The sessions consisted of  Wizard-Of-Oz experiments asking the subjects to solve different tasks without strong guidance. One Task given as an example was the planning of a trip to a cinema, including finding information about nearby cinemas and ordering of tickets. The different Scenarios varied between the sessions and were categorized by
			\begin{itemize}
				\item recording setup (Public, Home or Mobile)
				\item primary task
				\item secondary task
				\item available input methods
				\item recording environment (noise, background of frontal camera)
				\item user profile (gender, age, education, etc.)
			\end{itemize}
		\subsubsection{Experiment Setup}
				The system used for most data acquisition consisted of a graphical tablet onto which the Interface was projected, multiple Microphones as well as four cameras oriented to record facial expressions, a full-body side-view and the interaction area in color and infrared. The output of the system is through the projected graphical interface and synthesized speech and is controlled as an Wizard of Oz Experiment with two wizards in a separate Room controlling the experiment. 
				
		\subsection{Recorded Data}
		
		\Todo{Add first corpus}
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	
	
	\section{The CALLAS Expressivity Corpus}
		The yet to be released CALLAS Expressivity Corpus is part of the \gls{callas} project \cite{2010MultimodalCorpusForGestureExpressivity}. It was introduced in 2010 \cite{2010MultimodalCorpusForGestureExpressivity} and further described in \cite{Caridakis2013} three years later. All the information in this section is taken from those two papers if not stated otherwise, as this corpus was not released to the public yet, so there is no other information source besides a paper from one of the collaborators\cite{wagner:2011:1}. This corpus was a collaborative work between the National Technical University of Athens (G. Caridakis, A. Raouzaiou, K. Karpouzis), the Augsburg University (J. Wagner, F. Lingenfelser, E. Andre) and Humanware S.r.l. (Z. Curto). The available modalities are audio, video and hand gestures recorded separately in three different ways. 
		\subsection{Motivation}
			The main focus for this corpus was to capture cross-cultural differences in expressing emotions with hand gestures. Obtaining the data which is needed to investigate the differences in cultures was done by collecting data in Greece, Germany and Italy as well. Another motivation was the recording of material which is as realistic as possible. This led the team to use persons without any background as actor, with the result that the data's variance is high, which is better to model the real world as the emotion expression of humans may vary greatly.
		\subsection{Recording Scenario}
			The conducted experiment is split into three blocks which are then again split in three smaller parts. The difference between those bigger blocks is the emotion, which should be expressed by the proband. The method used to set the proband in a specific emotional mood was the Velten Mood Induction method \cite{VeltenJr1968473}. Using this method the user was free to modify the given sentence and was encouraged to make a gesture fitting the emotional mood. In the smaller sections three different gesture capturing methods were used.\\
			Many devices were used to record the data which are then incorporated into the corpus. The sentence to read aloud is projected onto the wall in front of the proband. Behind the user is a neutral background. Two cameras are positioned before the proband to capture the face and one to capture the whole upper body. They have a resolution of 720x576px, 24bit color depth and 25 fps. Audio was recorded using a microphone above the head with a samplerate of 16 kHz as mono. Gestures were recorded using a computer vision approaches to detect the hands and the head, a Nintendo Wii remote (often called Wiimote) for each hand or a data glove from Humanware. All datastreams were synchronized with a global clock by \gls{ssi} \cite{SmartSensorIntegration2010}.
		\subsection{Recorded Data}
			The corpus has 15 hours 14 minutes and 24 seconds of footage from all the three modalities. Audio is given by the sound which is recorded by the microphone. Video is available in two channels as two cameras were used for recording. One channel has shows the face and shoulder from the user. The other camera recorded the whole upper body including the hands, which is then used for gesture recognition. Gestures were captured by using three different methods. The first method was to use the video of the second camera which is then used to calculate the 2d position of the hand. The second method was to use the Wii remote to track the hands by capturing the data from the inbuilt accelerometer. The last approach to analyze the gestures executed by the probands is to use the data glove from Humanware which records the angles between the finger joints.
		\subsection{Application}
			The primary motivation for the creation of the Callas Expressivity Corpus was to study the cross cultural differences in gesturing under specific emotions. In both publications \cite{2010MultimodalCorpusForGestureExpressivity, Caridakis2013} the approach for this study was only briefly mentioned and was said to be future work. The only thing presented in this context was the feature extraction on the gesture channel of the corpus. Some of these features were overall activation, which represents how much the hands have moved executing a single gesture, the gesturing space, which estimates the space needed for the gesture, and fluidity, which separates abrubt from smooth and elegant gestures. The difficulty of this feature extraction was to find features, which could be calculated from all three capturing methods(bare hands, Wiimote and data glove) or at least finding features which describe similar things. For the computer vision method the movements of the hand inside of the video was used to derive the presented features. The data from the Wiimote could be used similarly to the data derived from the video as the position could be calculated from the accelerometer data recorded from it. The only difference is that with the Wii remote one can calculate the three dimensional position, which is not trivial for the first method. The third method used a data glove where the angles of the finger joints can be recorded. This was used to calculate similar features as the first two methods by using the movements of the fingers instead of the whole hand. \\
			Another use case for this corpus is to test the impact of modalities on emotion recognition, which is shortly described in \cite{Caridakis2013}. They used the audio and video streams from the corpus and examined the goodness of the classifier for four different combinations of features using the Leave-One-Speaker-Out approach. The fist classifier was only trained on audio, the second only on video, the third on both and the fourth classification was done with an decision fusion method. Decision fusion methods train a classifier for each modality and make the decision for a class using the predictions of the underlying classificators. This was tested on the German sub-copus as well as for the Italian sub-corpus. The results have shown that using all modalities or using a decision fusion approach increases the goodness of the classification compared to the cases where classificators were trained with a single modality. Another finding was that classificator which were trained on the Italian sub-corpus had a worse result when applied to the german sub-corpus than using the classifier trained on the German one. The same holds true for a classifier trained on the German parts and applied to the Italian one. Training on both languages delivered a worse result than the specifically trained classifier, but the difference was not too big, which then came with the advantage that the generalization was better. Another work from the German part of the team can be found in \cite{wagner:2011:1}, which describes a similar use case. There they have used the video, audio as well as the gestural modality and compared different feature fusion and decision fusion methods against another. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	
	
	\section{The Bielefeld Speech and Gesture Alignment Corpus ( SaGA )}
		The \gls{saga} corpus was produced by a research group at the University of Bielefeld and funded by the \gls{dfg}. It was carried out in the CRC 673 "Alignment in Communication" and published in the proceedings of the workshop "Multimodal Corpora-Advances in Capturing, Coding and Analyzing Multimodality" at the \gls{lrec} in 2010.
		
		The aim of the corpus was to portray the interplay of speech and gesture in face to face conversations. This task was taken on by a interdisicplinary approach. On the one hand the research team took a linguistic perspective to enable Theoretical Linguistic Reconstructions based on the corpus data, and on the other hand took the perspective of computer science to implement the lingusistic findings for multimodal communication in virtual agents and robots. \cite[ch. 1]{Bielefeld2010}
	
		\subsection{Recording Scenario}
			As the goal was to record speech in combination with gestures, the recording scenario had to chosen such that a sufficient amount of gestures was produced by the speaker. Furthermore the gestures should be easily interpretable to aid the annotation process and not externally forced to prevent unnatural ones. Therefore a scenario with the following properties was chosen \cite[ch. 2]{Bielefeld2010}:	
			\begin{itemize}
				\item The topic of the recorded converations was a spatial communication task. The participant had to give directions and describe landmarks. The descision for this topic was based on the evidence for a higher use of gestures, when speaking about spatial topics, provided by \cite[p. 313]{Alibali2005}. This ensured the appropriate amount of easily interpretable gestures.
				\item The stimulus for the participants was a "bus tour" through a virtual city. This was presented in \gls{vr} to make sure ensure the exact same stimulus for each participant. This improved the comparibility of the individual participant data.
				\item After receiving the stimulus, the participant had to describe the "bus tour" and the included landmarks to another person, who has not seen the stimulus. This provided a natural yet controlled conversation scenario for the participant.
			\end{itemize}	
						
		\subsection{Recorded Data}
			\subsubsection{Primary Data}
				The primary data comprises tree synchronized camera perspectives, the audio feed of the conversation, as well as body movement and eye tracking data of the direction giving participant. The camera perspectives include a frontal shot of the direction giver, one of the follower, and one of the whole dialog scenario. The specifications of the primary data given by \cite[ch. 2.1]{Bielefeld2010} and \cite{BAS2014} are summarized in \ref{table:sagaPrimary}. Further technical specifications of the primary data was not available.
				
				\begin{table}
					\center
					\begin{tabular}{|l|c|}
						\hline 
						\textbf{Specification} & \textbf{Value} \\ 
						\hline 
						Number of dialogs & 25 \\ 
						\hline 
						Video time & 280 minutes \\ 
						\hline 
						Video specification & 720x576, 25fps  \\ 
						\hline 
						Video codec & H.264  \\ 
						\hline 
						Video feeds & 3 \\ 
						\hline 
						Audio feeds & 1 \\ 
						\hline
					\end{tabular} 
					\caption{Primary data of the \gls{saga} corpus \cite{Bielefeld2010, BAS2014}}
					\label{table:sagaPrimary}
				\end{table}
				
			\subsubsection{Secondary Data}
				The secondary data of the corpus (\ref{table:sagaSecondary}) contains on the one hand the annotation of the speech and on the other hand the annotation of the gestures. The speech annotation transcribes the utterance on the level of words. Furthermore the utterance is grouped in clauses, each containing a communicative goal \cite[ch. 2]{Bielefeld2010}:
				\begin{itemize}
					\item Naming a landmark
					\item Landmark property description
					\item Landmark construction description
					\item Landmark position description
				\end{itemize}
				
				The gesture annotation has to be explained more in detail. The gestures of both the direction giver and the follower were annotated from two different perspectives. First the gestures were segmented and classified. Afterwards the morphology of the gestures was described. The classifying gesture annotation consists of annotation elements containing \cite[ch. 1.3]{Bergmann2014}:
				
				\begin{itemize}
					\item Annotation tier
					\item Start point in time
					\item End point in time
					\item Annotation value
				\end{itemize}								
				
				The tiers form a hierachy in the annotation and can contain different annotaion values:
				
				\begin{enumerate}
					\item The top tier is the \textbf{sequence}. A sequence contains several gestures directly following each other. There is no so called resting pose between them. No annotation value is assigned on this tier.
					\item The \textbf{phrase} is the tier of a single gesture. This gesture can have one of the following types:
					\begin{description}
						\item[deictic:] A static pointing gesture often involving the index finger. This gesture does not represent something by itself.
						\item[iconic:] A often dynamic gesture describing the object it represents.
						\item[beat:] A rhythmic gesture constisting of two phases. They are used to emphasize or structure the speech.
						\item[discourse:] A gesture used to structure the dialog between the partners.
						\item[move:] This are mostly unconscious movements depicting the emotional state of the subject.
						\item[mixed forms:] It is possible to assign mixed types consisting of the first three types.
					\end{description}
					\item The phrase can be divided in different \textbf{phases}. Following phases can be distinguished:
					\begin{itemize}
						\item Preparation
						\item Stroke
						\item Retraction
						\item Pre-hold
						\item Post-hold
					\end{itemize}
					\item The \textbf{practice} tier describes the meaning of a gesture by the way it is performed. Move, beat and discourse gestures are not annotated this way. Following practices can be assigned:
					\begin{description}
						\item[shaping] an object in the three-dimensional space.
						\item[sizing] an beforehands described object or depicting a distance.
						\item[modelling] an object. The hand itself is the representation of the object.
						\item[drawing] an object in the two-dimensional space.
						\item[pantomiming] an action the pantomime does himself.
						\item[indexing] an direction or to a position.
						\item[grasping] an object.
						\item[counting] with fingers or showing a number of fingers
						\item[hedging] is a methode displaying uncertainty.
					\end{description}					 
					\item The \textbf{perspective} tier describes the viewpoint from which a path is described by a gesture. If items are arranged relative to the speaker the 'speaker' value is assigned, the 'survey' value otherwise.
					\item The \textbf{referent type} tier contains the type of place, object, action, etc. the gesture referes to. Additionally it contains the \textbf{referent name} tier with the name of the reference.
				\end{enumerate}	
				
				The morphology annotation consists of the same annotation elements as the classifying one. The stroke of a gesture is described by the following properties \cite[ch. 2.2]{Bielefeld2010}
				\begin{itemize}
					\item The \textbf{hand shape} is classified by using a modified lexicon of the \gls{asl}.
					\item The \textbf{hand orientation} is modelled by the orientation of the \textbf{palm} and the \textbf{back of the hand}, the position of the \textbf{wrist} and its distance from the speaker.
					\item For dynamic movements the \textbf{movement path}, the \textbf{direction} and the \textbf{repitition} of the movement for each property are annotated. Additionally curved and linear movements are distinguished.
				\end{itemize}								
			
				\begin{table}
					\center
					\begin{tabular}{|l|c|}
						\hline
						\textbf{Specification} & \textbf{Value} \\ 
						\hline 
						Words & 39,435 \\ 
						\hline 
						Iconic/Deictic gestures & 4961 \\ 
						\hline 
						Discourse gestures & approx. 1000 \\ 
						\hline 
					\end{tabular} 
					\caption{Secondary data of the \gls{saga} corpus \cite{Bielefeld2010}}
				\label{table:sagaSecondary}
				\end{table}
	
		\subsection{Application}
			The authors of the \gls{saga} corpus present two different applications in \cite[ch. 4]{Bielefeld2010}.
			
			The first application is to create a \textbf{typological grid} based on the morphological gesture annotation. The aim of this application was to determine whether the shapes arising from the gestures are systematically used or if they are arbitrary tokens. Furthermore it has to be investigated if this systematic usage is only observed in one dialog, by only one participant or even in the whole corpus. With this additional information about the gestures, forming the connection between the meaning of the spoken language and the gestures can be simplified. The typology provides a more high level way of classifying gestures and gesture sequences. The findings of this research was published in \cite{Hahn2010}.
			
			The second usage of the corpus involves the automatic generation of iconic gestures for virtual Avatars. To do this the \gls{saga} corpus was used to learn \textbf{bayesian networks}. This networks can also be used to compare the gesture production of two individuals. The different propability distributions of the networks enable novel insights in the gesture generation process \cite[ch. 4.2]{Bielefeld2010}.
		
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  	\newpage
  	\eightpt
  	\bibliographystyle{IEEEtran}

  	\bibliography{thebib}

\end{document}
